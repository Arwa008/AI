{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw0jMDjlKoOY"
      },
      "outputs": [],
      "source": [
        "# Import needed libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas import option_context\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import Feature Extraction Methods\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#import classifiers\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "# import Evaluation Metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, f1_score,precision_score\n",
        "import math\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "pd.options.display.float_format = \"{:,.2f}\".format\n",
        "\n",
        "# import train and test set\n",
        "train = pd.read_csv('train_set.csv')\n",
        "train.columns = ['text','target']\n",
        "\n",
        "test = pd.read_csv('test_set.csv')\n",
        "test.columns = ['text','target']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0 : split dataset into Train_x , Train_y , Test_x , Test_y\n",
        "train_x = train['text']\n",
        "train_y = train['target']\n",
        "\n",
        "test_x = test['text']\n",
        "test_y = test['target']"
      ],
      "metadata": {
        "id": "DDghDKrlKpIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1 : local Factor 1 (TF)\n",
        "TF = CountVectorizer()\n",
        "\n",
        "count_TF = TF.fit_transform(train_x)\n",
        "featuresTF = TF.get_feature_names_out()\n",
        "train_TF = pd.DataFrame(data = count_TF.toarray(),columns = featuresTF)\n",
        "\n",
        "count_TFt = TF.transform(test_x)\n",
        "featuresTFt = TF.get_feature_names_out ()\n",
        "test_TF = pd.DataFrame(data = count_TFt.toarray(),columns = featuresTFt)"
      ],
      "metadata": {
        "id": "wJhnCLlFKpQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TFTDA Transformer"
      ],
      "metadata": {
        "id": "m7AmE0VYKy9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: TF-TDA\n",
        "class TF_TDA_t(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self,K):\n",
        "        self._K = K\n",
        "\n",
        "    def fit(self, X, y):\n",
        "   # find postive and negative tweets\n",
        "        X['label']=y\n",
        "        Positive_Training=X.loc[X['label'] == 1]\n",
        "        del Positive_Training['label']\n",
        "        Negative_Training=X.loc[X['label'] == 0]\n",
        "        del Negative_Training['label']\n",
        "        del X['label']\n",
        "#-----------------------------------------------------#\n",
        "   # find No. of training samples\n",
        "        N = len(X)\n",
        "#-----------------------------------------------------#\n",
        "   # find a and c\n",
        "        def FindA_C(Count,NoTweets,cls):\n",
        "            for i in Count.columns:\n",
        "                Count.loc[Count[i] > 0, i] = 1\n",
        "            data = []\n",
        "            for column in Count:\n",
        "                x=Count[column].sum()\n",
        "                y=x/NoTweets\n",
        "                y1=y*100\n",
        "                data.append((column,x,y1))\n",
        "            ranking = pd.DataFrame(data, columns=['Term','DF','cls_freq_'+cls])\n",
        "            tf=ranking.sort_values('cls_freq_'+cls, ascending=False)\n",
        "            index_names = tf[ tf['DF'] == 0 ].index\n",
        "            tf.drop(index_names, inplace = True)\n",
        "            return tf\n",
        "\n",
        "        a=FindA_C(Positive_Training,len(Positive_Training),'p')\n",
        "        c=FindA_C(Negative_Training,len(Negative_Training),'n')\n",
        "        self._a = a\n",
        "        self._c = c\n",
        "\n",
        "#-----------------------------------------------------#\n",
        "        # find positive and negative percentages\n",
        "        PosPer = round((len(Positive_Training)/N),2)\n",
        "        #print(PosPer)\n",
        "        self._PosPer = PosPer\n",
        "        NegPer = round((len(Negative_Training)/N),2)\n",
        "        #print(NegPer)\n",
        "        self._NegPer = NegPer\n",
        "\n",
        "#------------------Find T+, T-, and Com------------------#\n",
        "        #find common terms between pos and neg (a and c)\n",
        "        commonTermsTraining = a.merge(c, on=['Term'])\n",
        "        commonTermsTraining.columns=['Term','DFP','cls_freq_p','DFN','cls_freq_n']\n",
        "        AllCommon = [x for x in commonTermsTraining['Term']]\n",
        "        self._AllCommon = AllCommon\n",
        "        #print('All common ',len(AllCommon))\n",
        "\n",
        "        # Find Pure Terms (T+, T-)\n",
        "        PurePos = [x for x in a['Term'] if x not in AllCommon]\n",
        "        #print('PurePos ',len(PurePos))\n",
        "        PureNeg = [x1 for x1 in c['Term'] if x1 not in AllCommon]\n",
        "        #print('PureNeg ',len(PureNeg))\n",
        "        self._PureNeg = PureNeg\n",
        "        self._PurePos = PurePos\n",
        "\n",
        "        K = self._K\n",
        "#------------------ Find Freq+, Freq-, and G ------------------#\n",
        "\n",
        "        # find the variacne between cls_freq_p and cls_freq_n\n",
        "        commonTermsTraining['Variance'] = commonTermsTraining['cls_freq_p']-commonTermsTraining['cls_freq_n']\n",
        "\n",
        "        #common Negative (Freq-)\n",
        "        commonTermsTrainingNegative = commonTermsTraining.loc[commonTermsTraining['Variance'] <=(-1*K)]\n",
        "        CommonNegative=[x for x in commonTermsTrainingNegative.Term]\n",
        "        self._CommonNegative=CommonNegative\n",
        "\n",
        "        #common Positive (Freq+)\n",
        "        commonTermsTrainingPositive = commonTermsTraining.loc[commonTermsTraining['Variance'] >=K]\n",
        "        CommonPositive=[x for x in commonTermsTrainingPositive.Term]\n",
        "        self._CommonPositive=CommonPositive\n",
        "\n",
        "        # useful common terms (UsefulCommon)\n",
        "        UsefulCommon=CommonPositive+CommonNegative\n",
        "        self._UsefulCommon=UsefulCommon\n",
        "        #find General terms\n",
        "        G = [x for x in AllCommon if x not in UsefulCommon]\n",
        "        self._G = G\n",
        "        #print('G ',len(G))\n",
        "\n",
        "        data = []\n",
        "        data.append((K,len(CommonNegative),len(CommonPositive),len(G),len(PureNeg),len(PurePos)))\n",
        "        data1 = pd.DataFrame(data, columns=['K','CommonNeg','CommonPos','G','PureNeg','PurePos'])\n",
        "        #display(data1)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        G = self._G\n",
        "\n",
        "        CommonNegative=self._CommonNegative\n",
        "        CommonPositive=self._CommonPositive\n",
        "\n",
        "        UsefulCommon=self._UsefulCommon\n",
        "        AllCommon=self._AllCommon\n",
        "\n",
        "        a = self._a\n",
        "        c = self._c\n",
        "\n",
        "        NegPer = self._NegPer\n",
        "        PosPer = self._PosPer\n",
        "\n",
        "        PureNeg = self._PureNeg\n",
        "        PurePos = self._PurePos\n",
        "\n",
        "        def method (cls_freq_a,cls_freq_c,x,cls_per):\n",
        "            MAX = max(1,cls_freq_c)\n",
        "            k=2+((cls_freq_a/MAX)*(x*cls_per))\n",
        "            rf = math.log2(k)\n",
        "            return rf\n",
        "\n",
        "        for i in UsefulCommon:\n",
        "            a1 = a.loc[a['Term'] == i, 'cls_freq_p'].item()\n",
        "            c1 = c.loc[c['Term'] == i, 'cls_freq_n'].item()\n",
        "            if i in CommonPositive:\n",
        "                X[i] = X[i]*(method(a1,c1,1,PosPer))\n",
        "            elif i in CommonNegative:\n",
        "                X[i]= X[i]*(method(c1,a1,1,NegPer))\n",
        "\n",
        "        for p in PurePos:\n",
        "            a1 = a.loc[a['Term'] == p, 'cls_freq_p'].item()\n",
        "            c1 = 0\n",
        "            X[p]= X[p]*(method(a1,c1,2,PosPer))\n",
        "\n",
        "        for n in PureNeg:\n",
        "            c1 = c.loc[c['Term'] == n, 'cls_freq_n'].item()\n",
        "            a1 = 0\n",
        "            X[n]= X[n]*(method(c1,a1,2,NegPer))\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "qYr_CoN3KpTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling and Evaluation Results\n"
      ],
      "metadata": {
        "id": "2e0RuNHeLCBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame()\n",
        "\n",
        "# Classification Models\n",
        "NB = MultinomialNB()\n",
        "SVM = SVC(kernel = 'linear')\n",
        "\n",
        "def Classification_Process(clf,method,t):\n",
        "    test_x_Copy = test_TF.copy()\n",
        "    train_x_Copy = train_TF.copy()\n",
        "    Transformer = t\n",
        "    train_TFIDF1 = Transformer.fit_transform(train_x_Copy,train_y)\n",
        "    test_TFIDF1= Transformer.transform(test_x_Copy)\n",
        "    clf.fit(train_TFIDF1,train_y)\n",
        "    predM = clf.predict(test_TFIDF1)\n",
        "    Precision = '{:.2%}'.format(precision_score(test_y, predM,average='weighted'))\n",
        "    Recall = '{:.2%}'.format(recall_score(test_y, predM,average='weighted'))\n",
        "    F_Score = '{:.2%}'.format(f1_score(test_y, predM,average='weighted'))\n",
        "    accuracy = '{:.2%}'.format(accuracy_score(test_y, predM))\n",
        "    results1=results.append({'clf':clf,'Method':method,'precision':Precision,'recall':Recall,'F_Score':F_Score,'Accuracy':accuracy},ignore_index=True)\n",
        "    return results1"
      ],
      "metadata": {
        "id": "p0PY3aeqKpWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Results of NB & SVM\n"
      ],
      "metadata": {
        "id": "i2lWhxyfLHuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that x is the optimal k value which may differ based the dataset used and classification model\n",
        "optimal_k = x\n",
        "TF_TDA_NB = Classification_Process(NB,'TF_TDA',TF_TDA_t(optimal_k))\n",
        "TF_TDA_SVM = Classification_Process(SVM,'TF_TDA',TF_TDA_t(optimal_k))\n",
        "\n",
        "frames = [TF_TDA_NB,TF_TDA_SVM]\n",
        "Final_result = pd.concat(frames)\n",
        "display(Final_result)"
      ],
      "metadata": {
        "id": "iPpeg9nSKpYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p9NrE72yKpbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmmxjbO5Kpdw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}